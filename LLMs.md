# Large Language models timeline and key papers
[Transformers, the tech behind LLMs](https://www.youtube.com/watch?v=wjZofJX0v4M) & [Attention in transformers](https://www.youtube.com/watch?v=eMlx5fFNoYc)
## 1. Foundational Theory and Early Pioneering Works
- **2017 arXiv(NeurIPS 2020): “Attention Is All You Need” (Vaswani et al.)**

  > [Paper](https://arxiv.org/abs/1706.03762) & [Openreview](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Reviews.html) & [Code(Original Tensorflow version)](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py) & [Code(Pytorch version)](https://github.com/jadore801120/attention-is-all-you-need-pytorch)

## 2. Core Large Language models
- **2020 arXiv(NeurIPS 2020): “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks” (Lewis et al.)**

  > [Paper](https://arxiv.org/abs/2005.11401) & [Video](https://www.youtube.com/watch?v=JGpmQvlYRdU)

- **2021 arXiv(ICLR 2022): “LoRA: Low-Rank Adaptation of Large Language Models” (J. Hu et al.)**

  > [Paper](https://arxiv.org/abs/2106.09685) & [OpenReview](https://openreview.net/forum?id=nZeVKeeFYf9) & [Video](https://www.youtube.com/watch?v=DhRoTONcyZE) & [Code](https://github.com/microsoft/LoRA)
